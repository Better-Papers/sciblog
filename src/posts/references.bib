@article{gaborTheoryCommunication1946,
  title      = {Theory of Communication. {{Part}} 1: {{The}} Analysis of Information},
  shorttitle = {Theory of Communication. {{Part}} 1},
  author     = {Gabor, D.},
  year       = {1946},
  month      = nov,
  journal    = {Journal of the Institution of Electrical Engineers - Part III: Radio and Communication Engineering},
  volume     = {93},
  number     = {26},
  pages      = {429--441},
  publisher  = {{IET Digital Library}},
  issn       = {2054-0604},
  doi        = {10.1049/ji-3-2.1946.0074},
  abstract   = {Hitherto communication theory was based on two alternative methods of signal analysis. One is the description of the signal as a function of time; the other is Fourier analysis. Both are idealizations, as the first method operates with sharply defined instants of time, the second with infinite wave-trains of rigorously defined frequencies. But our everyday experiences\textemdash especially our auditory sensations\textemdash insist on a description in terms of both time and frequency. In the present paper this point of view is developed in quantitative language. Signals are represented in two dimensions, with time and frequency as co-ordinates. Such two-dimensional representations can be called ``information diagrams,'' as areas in them are proportional to the number of independent data which they can convey. This is a consequence of the fact that the frequency of a signal which is not of infinite duration can be defined only with a certain inaccuracy, which is inversely proportional to the duration, and vice versa. This ``uncertainty relation'' suggests a new method of description, intermediate between the two extremes of time analysis and spectral analysis. There are certain ``elementary signals'' which occupy the smallest possible area in the information diagram. They are harmonic oscillations modulated by a ``probability pulse.'' Each elementary signal can be considered as conveying exactly one datum, or one ``quantum of information.'' Any signal can be expanded in terms of these by a process which includes time analysis and Fourier analysis as extreme cases.These new methods of analysis, which involve some of the mathematical apparatus of quantum theory, are illustrated by application to some problems of transmission theory, such as direct generation of single sidebands, signals transmitted in minimum time through limited frequency channels, frequency modulation and time-division multiplex telephony.},
  langid     = {english}
}

@article{jonasCouldNeuroscientist2017,
  title     = {Could a {{Neuroscientist Understand}} a {{Microprocessor}}?},
  author    = {Jonas, Eric and Kording, Konrad Paul},
  year      = {2017},
  month     = jan,
  journal   = {PLOS Computational Biology},
  volume    = {13},
  number    = {1},
  pages     = {e1005268},
  publisher = {{Public Library of Science}},
  issn      = {1553-7358},
  doi       = {10.1371/journal.pcbi.1005268},
  abstract  = {There is a popular belief in neuroscience that we are primarily data limited, and that producing large, multimodal, and complex datasets will, with the help of advanced data analysis algorithms, lead to fundamental insights into the way the brain processes information. These datasets do not yet exist, and if they did we would have no way of evaluating whether or not the algorithmically-generated insights were sufficient or even correct. To address this, here we take a classical microprocessor as a model organism, and use our ability to perform arbitrary experiments on it to see if popular data analysis methods from neuroscience can elucidate the way it processes information. Microprocessors are among those artificial information processing systems that are both complex and that we understand at all levels, from the overall logical flow, via logical gates, to the dynamics of transistors. We show that the approaches reveal interesting structure in the data but do not meaningfully describe the hierarchy of information processing in the microprocessor. This suggests current analytic approaches in neuroscience may fall short of producing meaningful understanding of neural systems, regardless of the amount of data. Additionally, we argue for scientists using complex non-linear dynamical systems with known ground truth, such as the microprocessor as a validation platform for time-series and structure discovery methods.},
  langid    = {english},
  keywords  = {Behavior,Behavioral neuroscience,Computational neuroscience,Connectomics,Microprocessors,Neuronal tuning,Neurons,Neuroscience}
}

@article{kaplanExplanatoryForce2011,
  title      = {The {{Explanatory Force}} of {{Dynamical}} and {{Mathematical Models}} in {{Neuroscience}}: {{A Mechanistic Perspective}}*},
  shorttitle = {The {{Explanatory Force}} of {{Dynamical}} and {{Mathematical Models}} in {{Neuroscience}}},
  author     = {Kaplan, David Michael and Craver, Carl F.},
  year       = {2011},
  journal    = {Philosophy of Science},
  volume     = {78},
  number     = {4},
  pages      = {601--627},
  publisher  = {{[The University of Chicago Press, Philosophy of Science Association]}},
  issn       = {0031-8248},
  doi        = {10.1086/661755},
  abstract   = {We argue that dynamical and mathematical models in systems and cognitive neuroscience explain (rather than redescribe) a phenomenon only if there is a plausible mapping between elements in the model and elements in the mechanism for the phenomenon. We demonstrate how this model-to-mechanism-mapping constraint, when satisfied, endows a model with explanatory force with respect to the phenomenon to be explained. Several paradigmatic models including the Haken-Kelso-Bunz model of bimanual coordination and the difference-of-Gaussians model of visual receptive fields are explored.}
}