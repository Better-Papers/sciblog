---
title: "Multivariate Normal Conjugacy"
section: "Bayesian Statistics"
author: "Chaichontat Sriworarat"
date: 2019-10-15
tags: ["bayesian statistics"]
include-before:
  - '$\newcommand{\summ}{\sum_{i=1}^n}$'
  - '$\newcommand{\prodd}{\prod_{i=1}^n}$'
  - '$\newcommand{\xdots}{x_1, \dots, x_n}$'
  - '$\newcommand{\expb}[1]{\exp\left\{#1\right\}}$'
  - '$\newcommand{\pp}[1]{\left(#1\right)}$'
  - '$\newcommand{\b}[1]{\mathbf{#1}}$'
  - '$\newcommand{\bs}[1]{\boldsymbol{#1}}$'
  - '$\newcommand{\bxdots}{\b{x}_1,\dots,\b{x}_n}$'
---


<span class="math inline">\(\newcommand{\summ}{\sum_{i=1}^n}\)</span>
<span class="math inline">\(\newcommand{\prodd}{\prod_{i=1}^n}\)</span>
<span class="math inline">\(\newcommand{\xdots}{x_1, \dots, x_n}\)</span>
<span class="math inline">\(\newcommand{\expb}[1]{\exp\left\{#1\right\}}\)</span>
<span class="math inline">\(\newcommand{\pp}[1]{\left(#1\right)}\)</span>
<span class="math inline">\(\newcommand{\b}[1]{\mathbf{#1}}\)</span>
<span class="math inline">\(\newcommand{\bs}[1]{\boldsymbol{#1}}\)</span>
<span class="math inline">\(\newcommand{\bxdots}{\b{x}_1,\dots,\b{x}_n}\)</span>

<div id="the-multivariate-normal" class="section level1">
<h1>The Multivariate Normal</h1>
<p>Recall the univariate normal
<span class="math display">\[
\begin{align}
p(x\mid μ,σ^2) &amp;= (2πσ^2)^{-1/2} \, \expb{-\frac{(x-μ)^2}{2\sigma^2}}\\
&amp;= (2\pi)^{-1/2}(\sigma^2)^{-1/2}\, \expb{-\frac{1}{2}(x-\mu)(\sigma^2)^{-1}(x-\mu)}
\end{align}
\]</span>
This is the multivariate normal.
<span class="math display">\[
p(\b{x}\mid\bs{\mu, \Sigma}) = (2\pi)^{-k/2}\ \mathrm{det}(\bs{\Sigma})^{-1/2} \, \expb{-\frac{1}{2}(\b{x}-\bs{\mu})^\top\bs{\Sigma}^{-1}(\b{x}-\bs{\mu})}
\]</span></p>
<p>Note that <span class="math inline">\(\b{x^\top Ax}\)</span> is the multivariate analogue of <span class="math inline">\(Ax^2\)</span>. The normalization constants are also updated to handle multidimensional data.</p>
</div>
<div id="infer-bsmu" class="section level1">
<h1>Infer <span class="math inline">\(\bs\mu\)</span></h1>
<p>In the univariate case, we figured that the univariate normal is the prior.
Let <span class="math inline">\(\bs{\mu}_0\)</span> be the prior mean of length <span class="math inline">\(k\)</span> and <span class="math inline">\(\bs{\Lambda}_0\)</span> be the prior covariance matrix.
<span class="math display">\[
\begin{align}
p(\bs{\mu} \mid \bs{\mu}_0, \bs{\Lambda}_0) &amp;= (2\pi)^{-k/2}\ \mathrm{det}(\bs{\Lambda}_0)^{-1/2} \, \expb{-\frac{1}{2}(\bs{\mu}-\bs{\mu_0})^\top\bs{\Lambda}_0^{-1}(\bs{\mu}-\bs{\mu_0})} \\
&amp;\propto \expb{-\frac{1}{2}\left[\bs{\mu}^\top \bs{\Lambda}_0^{-1} \bs{\mu} -2\bs{\mu}^\top\bs{\Lambda}_0^{-1}\bs{\mu_0}+\bs{\mu_0}^\top\bs{\Lambda}_0^{-1}\bs{\mu_0}\right]} \\
&amp;\propto \expb{-\frac{1}{2} \bs{\mu}^\top \bs{\Lambda}_0^{-1} \bs{\mu} + \bs{\mu}^\top \bs{\Lambda}_0^{-1} \bs{\mu_0}} \\
\end{align}
\]</span></p>
<p>Standard form
<span class="math display">\[
\begin{align}
   \mathrm{MultivariateNormal}\pp{\b{A}^{-1}\b{b}, \b{A}^{-1}} \propto \expb{- \frac{1}{2} \bs{\mu}^\top\b{A} \bs{\mu} + \bs{\mu}^\top \b{b}}
\end{align}
\]</span></p>
<p>Likelihood
<span class="math display">\[
\begin{align}
p(\b{x}_1,\dots,\b{x}_n \mid \bs{\mu}, \bs{\Sigma}) &amp;= \prodd (2\pi)^{-k/2}\ \mathrm{det}(\bs{\Sigma})^{-1/2} \, \expb{-\frac{1}{2}(\b{x}_i-\bs{\mu})^\top\bs{\Sigma}^{-1}(\b{x}_i-\bs{\mu})} \\
&amp;\propto \expb{-\frac{1}{2}\summ\left[\b{x}_i^\top \bs{\Sigma}^{-1} \b{x}_i -2\b{x}_i^\top\bs{\Sigma}^{-1}\bs{\mu}+\bs{\mu}^\top\bs{\Sigma}^{-1}\bs{\mu}\right]} \\
&amp;\propto \expb{-\frac{1}{2} \summ\left[\bs{\mu}^\top \bs{\Sigma}^{-1} \bs{\mu} + \b{x}^\top \bs{\Sigma}^{-1} \bs{\mu}\right]} \\
&amp;\propto \expb{-\frac{1}{2} \bs{\mu}^\top n\bs{\Sigma}^{-1} \bs{\mu} + \summ \bs\mu^\top \bs{\Sigma}^{-1} \b{x}} \\
&amp;\propto \mathrm{MultivariateNormal}\pp{\bs{\mu} \mid \bar{\b{x}}, \frac{\bs{\Sigma}}{n}}
\end{align}
\]</span></p>
<p>Posterior
<span class="math display">\[
\begin{align}
  p(\bs{\mu} \mid \bxdots, \bs{\Sigma}, \boldsymbol{\mu}_0 \bs{\Lambda}_0) &amp;\propto p(\bxdots, \mid \boldsymbol{\mu}, \bs{\Sigma})\ p(\boldsymbol{\mu}\mid \bs{\mu}_0, \bs{\Lambda}_0) \\
  &amp;\propto \expb{-\frac{1}{2} \bs{\mu}^\top n\bs{\Sigma}^{-1} \bs{\mu} + \summ \bs\mu^\top \bs{\Sigma}^{-1} \b{x}_i} \
    \expb{-\frac{1}{2} \bs{\mu}^\top \bs{\Lambda}_0^{-1} \bs{\mu} + \bs\mu^\top \bs{\Lambda}_0^{-1} \boldsymbol{\mu}_0 } \\
  &amp;\propto \expb{- \frac{1}{2} \boldsymbol{\mu}^\top(n\bs{\Sigma}^{-1}+\bs{\Lambda}_0^{-1}) \boldsymbol{\mu} + \boldsymbol{\mu}^\top(n \boldsymbol{\Sigma}^{-1} \bar{\mathbf{x}} + \boldsymbol{\Lambda}_0^{-1} \boldsymbol{\mu}_0  ) } \\
  &amp;\propto \mathrm{MultivariateNormal}\pp{ \boldsymbol{\mu}\mid \frac{n \boldsymbol{\Sigma}^{-1} \bar{\mathbf{x}} + \boldsymbol{\Lambda}_0^{-1} \boldsymbol{\mu}_0}{n\bs{\Sigma}^{-1}+\bs{\Lambda}_0^{-1}}, \frac{1}{n\bs{\Sigma}^{-1}+\bs{\Lambda}_0^{-1}}}
\end{align}
\]</span></p>
</div>
<div id="infer-bssigma" class="section level1">
<h1>Infer <span class="math inline">\(\bs\Sigma\)</span></h1>
<p>This is weird.</p>
<p>In the univariate case, we could derive the prior directly like we have done above. However, this is more complicated in the multivariate case, so we’re going to derive this from first principles.</p>
<p>Desiderata for a covariance matrix</p>
<ul>
<li>Symmetric (<span class="math inline">\(\sigma_{XY}^2=\sigma_{YX}^2\)</span>)</li>
<li>Positive semi-definite (<span class="math inline">\(-1 \leq\rho\leq 1\)</span> and <span class="math inline">\(\sigma \geq 0\)</span>)</li>
</ul>
<p>Using the variance formula, given a mean-centered matrix $  $, then the covariance matrix is
<span class="math display">\[
\boldsymbol{\Sigma} = \frac{1}{n-1} \mathbf{Z}^\top \mathbf{Z}  
\]</span></p>
<p>Then to generate a distribution of random covariance matrices given a covariance matrix,</p>
<p><span class="math display">\[
\begin{align}
  \mathbf{Z} &amp;\sim \mathrm{MultivariateNormal} ( \boldsymbol{0}, \boldsymbol{\Sigma}_0  ) \\
  \boldsymbol{\Sigma}_0,\dots,\boldsymbol{\Sigma}_{n_0} &amp;\overset{\text{i.i.d.}}{\sim} \mathbf{Z^\top Z} \\
  &amp;\sim \mathrm{InverseWishart}(n_0, \boldsymbol{\Sigma}_0^{-1} )
\end{align}
\]</span>
where <span class="math inline">\(n_0\)</span> is a number larger than <span class="math inline">\(k\)</span> (length of sample).</p>
<p><span class="math display">\[
\begin{align}
\mathcal{L}(\bs{\Sigma}\mid \b{x}_1,\dots,\b{x}_n) &amp;=p(\b{x}_1,\dots,\b{x}_n \mid \bs{\mu}, \bs{\Sigma}) \\
&amp;= \prodd (2\pi)^{-k/2}\ \mathrm{det}(\bs{\Sigma})^{-1/2} \, \expb{-\frac{1}{2}(\b{x}_i-\bs{\mu})^\top\bs{\Sigma}^{-1}(\b{x}_i-\bs{\mu})} \\
&amp;\propto \mathrm{det}(\bs{\Sigma})^{-n/2}\, \expb{-\frac{1}{2}\summ (\b{x}_i-\bs{\mu})^\top\bs{\Sigma}^{-1}(\b{x}_i-\bs{\mu})} \\
&amp;\propto \mathrm{det}(\bs{\Sigma})^{-n/2}\, \expb{-\frac{1}{2} \operatorname{tr}\pp{\summ \left[(\b{x}_i-\bs{\mu})^\top(\b{x}_i-\bs{\mu})\right]\bs{\Sigma}^{-1}}} \\
&amp;\propto \mathrm{InverseWishart}\pp{n+p+1, \summ (\b{x}_i-\bs{\mu})^\top(\b{x}_i-\bs{\mu})}
\end{align}
\]</span>
This is the Wishart distribution.
<span class="math display">\[
p (\bs{\Lambda}\mid \b{V}) = \frac{1}{2^{np/2} \left|{\mathbf V}\right|^{n/2} \Gamma_p\left(\frac {n}{2}\right ) }{\left|\bs{\Lambda}\right|}^{(n-p-1)/2} \expb{-\frac{1}{2}\operatorname{tr}({\mathbf V}^{-1}\bs{\Lambda})}
\]</span>
However, looking at the likelihood form, every parameter of interest is inverted. Therefore, it is conjugated to the inverse Wishart distribution.</p>
<p><span class="math display">\[
p (\bs{\Sigma}\mid \b{V}) = \frac{1}{2^{np/2} \left|{\mathbf V}\right|^{-n/2} \Gamma_p\left(\frac {n}{2}\right ) }{\left|\bs{\Sigma}\right|}^{-(n-p-1)/2} \expb{-\frac{1}{2}\operatorname{tr}({\mathbf V}\bs{\Sigma}^{-1})}
\]</span></p>
<p>In the univariate case this would be
??????
<span class="math display">\[
\begin{align}
  X &amp;\sim \mathcal{N}(0, \tau_0^2) \\
  \sigma^2 \mid \tau^2_0 &amp;\sim X^2 \\
  &amp;\sim \tau_0^2\, \mathcal{N}(0,1)^2 \\
  &amp;\sim \tau_0^2\, \chi^2_1 \\
  &amp;\sim \tau_0^2\, \mathrm{Gamma}\pp{\frac{1}{2}, \frac{1}{2} } \\
  &amp;\sim \mathrm{Gamma}\pp{\frac{1}{2}, \frac{1}{2\tau_0^2} }
\end{align}
\]</span>
<span class="math display">\[
\summ \mathbf{b}_i^\top \mathbf{Ab}_i = \mathrm{tr}( \mathbf{B^\top B A} )  
\]</span></p>
</div>
<div id="infer-both" class="section level1">
<h1>Infer Both</h1>
<p>Here, we are seeking to infer both the mean and variance of data distributed with a multivariate normal sampling distribution.</p>
<div id="full-conditional-of-bssigma" class="section level2">
<h2>Full conditional of <span class="math inline">\(\bs{\Sigma}\)</span></h2>
<p>We know that the sampling distribution of the covariance matrix is conjugated to the inverse Wishart distribution.</p>
<p><span class="math display">\[
\begin{align}
  p(\b{X}_1,\dots,\b{X}_n) &amp;\propto 
\end{align}
\]</span></p>
</div>
<div id="sampling-scheme" class="section level2">
<h2>Sampling scheme</h2>
<p><span class="math display">\[
\begin{align}
  \b{μ}\mid\b{X}_1,\dots,\b{X}_n, \b{Σ} &amp;\sim \mathrm{MultivariateNormal}(\b{μ}_n, \b{Λ}_n) \\
  \b{Σ} \mid \b{X}_1,\dots,\b{X}_n, \b{μ} &amp;\sim \mathrm{InverseWishart}(n_0+n, \b{Σ}_n^{-1})\\
\end{align}
\]</span></p>
</div>
</div>
