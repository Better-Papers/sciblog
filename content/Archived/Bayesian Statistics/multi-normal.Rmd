---
title: "Multivariate Normal Conjugacy"
section: "Bayesian Statistics"
author: "Chaichontat Sriworarat"
date: 2019-10-15
tags: ["bayesian statistics"]
include-before:
  - '$\newcommand{\summ}{\sum_{i=1}^n}$'
  - '$\newcommand{\prodd}{\prod_{i=1}^n}$'
  - '$\newcommand{\xdots}{x_1, \dots, x_n}$'
  - '$\newcommand{\expb}[1]{\exp\left\{#1\right\}}$'
  - '$\newcommand{\pp}[1]{\left(#1\right)}$'
  - '$\newcommand{\b}[1]{\mathbf{#1}}$'
  - '$\newcommand{\bs}[1]{\boldsymbol{#1}}$'
  - '$\newcommand{\bxdots}{\b{x}_1,\dots,\b{x}_n}$'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
library(blogdown)
```

# The Multivariate Normal
Recall the univariate normal
$$
\begin{align}
p(x\mid μ,σ^2) &= (2πσ^2)^{-1/2} \, \expb{-\frac{(x-μ)^2}{2\sigma^2}}\\
&= (2\pi)^{-1/2}(\sigma^2)^{-1/2}\, \expb{-\frac{1}{2}(x-\mu)(\sigma^2)^{-1}(x-\mu)}
\end{align}
$$
This is the multivariate normal.
$$
p(\b{x}\mid\bs{\mu, \Sigma}) = (2\pi)^{-k/2}\ \mathrm{det}(\bs{\Sigma})^{-1/2} \, \expb{-\frac{1}{2}(\b{x}-\bs{\mu})^\top\bs{\Sigma}^{-1}(\b{x}-\bs{\mu})}
$$

Note that $\b{x^\top Ax}$ is the multivariate analogue of $Ax^2$. The normalization constants are also updated to handle multidimensional data.

# Infer $\bs\mu$
In the univariate case, we figured that the univariate normal is the prior.
Let $\bs{\mu}_0$ be the prior mean of length $k$ and $\bs{\Lambda}_0$ be the prior covariance matrix.
$$
\begin{align}
p(\bs{\mu} \mid \bs{\mu}_0, \bs{\Lambda}_0) &= (2\pi)^{-k/2}\ \mathrm{det}(\bs{\Lambda}_0)^{-1/2} \, \expb{-\frac{1}{2}(\bs{\mu}-\bs{\mu_0})^\top\bs{\Lambda}_0^{-1}(\bs{\mu}-\bs{\mu_0})} \\
&\propto \expb{-\frac{1}{2}\left[\bs{\mu}^\top \bs{\Lambda}_0^{-1} \bs{\mu} -2\bs{\mu}^\top\bs{\Lambda}_0^{-1}\bs{\mu_0}+\bs{\mu_0}^\top\bs{\Lambda}_0^{-1}\bs{\mu_0}\right]} \\
&\propto \expb{-\frac{1}{2} \bs{\mu}^\top \bs{\Lambda}_0^{-1} \bs{\mu} + \bs{\mu}^\top \bs{\Lambda}_0^{-1} \bs{\mu_0}} \\
\end{align}
$$

Standard form
$$
\begin{align}
   \mathrm{MultivariateNormal}\pp{\b{A}^{-1}\b{b}, \b{A}^{-1}} \propto \expb{- \frac{1}{2} \bs{\mu}^\top\b{A} \bs{\mu} + \bs{\mu}^\top \b{b}}
\end{align}
$$

Likelihood
$$
\begin{align}
p(\b{x}_1,\dots,\b{x}_n \mid \bs{\mu}, \bs{\Sigma}) &= \prodd (2\pi)^{-k/2}\ \mathrm{det}(\bs{\Sigma})^{-1/2} \, \expb{-\frac{1}{2}(\b{x}_i-\bs{\mu})^\top\bs{\Sigma}^{-1}(\b{x}_i-\bs{\mu})} \\
&\propto \expb{-\frac{1}{2}\summ\left[\b{x}_i^\top \bs{\Sigma}^{-1} \b{x}_i -2\b{x}_i^\top\bs{\Sigma}^{-1}\bs{\mu}+\bs{\mu}^\top\bs{\Sigma}^{-1}\bs{\mu}\right]} \\
&\propto \expb{-\frac{1}{2} \summ\left[\bs{\mu}^\top \bs{\Sigma}^{-1} \bs{\mu} + \b{x}^\top \bs{\Sigma}^{-1} \bs{\mu}\right]} \\
&\propto \expb{-\frac{1}{2} \bs{\mu}^\top n\bs{\Sigma}^{-1} \bs{\mu} + \summ \bs\mu^\top \bs{\Sigma}^{-1} \b{x}} \\
&\propto \mathrm{MultivariateNormal}\pp{\bs{\mu} \mid \bar{\b{x}}, \frac{\bs{\Sigma}}{n}}
\end{align}
$$

Posterior
$$
\begin{align}
  p(\bs{\mu} \mid \bxdots, \bs{\Sigma}, \boldsymbol{\mu}_0 \bs{\Lambda}_0) &\propto p(\bxdots, \mid \boldsymbol{\mu}, \bs{\Sigma})\ p(\boldsymbol{\mu}\mid \bs{\mu}_0, \bs{\Lambda}_0) \\
  &\propto \expb{-\frac{1}{2} \bs{\mu}^\top n\bs{\Sigma}^{-1} \bs{\mu} + \summ \bs\mu^\top \bs{\Sigma}^{-1} \b{x}_i} \
    \expb{-\frac{1}{2} \bs{\mu}^\top \bs{\Lambda}_0^{-1} \bs{\mu} + \bs\mu^\top \bs{\Lambda}_0^{-1} \boldsymbol{\mu}_0 } \\
  &\propto \expb{- \frac{1}{2} \boldsymbol{\mu}^\top(n\bs{\Sigma}^{-1}+\bs{\Lambda}_0^{-1}) \boldsymbol{\mu} + \boldsymbol{\mu}^\top(n \boldsymbol{\Sigma}^{-1} \bar{\mathbf{x}} + \boldsymbol{\Lambda}_0^{-1} \boldsymbol{\mu}_0  ) } \\
  &\propto \mathrm{MultivariateNormal}\pp{ \boldsymbol{\mu}\mid \frac{n \boldsymbol{\Sigma}^{-1} \bar{\mathbf{x}} + \boldsymbol{\Lambda}_0^{-1} \boldsymbol{\mu}_0}{n\bs{\Sigma}^{-1}+\bs{\Lambda}_0^{-1}}, \frac{1}{n\bs{\Sigma}^{-1}+\bs{\Lambda}_0^{-1}}}
\end{align}
$$

# Infer $\bs\Sigma$
This is weird.

In the univariate case, we could derive the prior directly like we have done above. However, this is more complicated in the multivariate case, so we're going to derive this from first principles.

Desiderata for a covariance matrix

- Symmetric ($\sigma_{XY}^2=\sigma_{YX}^2$)
- Positive semi-definite ($-1 \leq\rho\leq 1$ and $\sigma \geq 0$)

Using the variance formula, given a mean-centered matrix $ \mathbf{Z} $, then the covariance matrix is
$$
\boldsymbol{\Sigma} = \frac{1}{n-1} \mathbf{Z}^\top \mathbf{Z}  
$$

Then to generate a distribution of random covariance matrices given a covariance matrix,

$$
\begin{align}
  \mathbf{Z} &\sim \mathrm{MultivariateNormal} ( \boldsymbol{0}, \boldsymbol{\Sigma}_0  ) \\
  \boldsymbol{\Sigma}_0,\dots,\boldsymbol{\Sigma}_{n_0} &\overset{\text{i.i.d.}}{\sim} \mathbf{Z^\top Z} \\
  &\sim \mathrm{InverseWishart}(n_0, \boldsymbol{\Sigma}_0^{-1} )
\end{align}
$$
where $n_0$ is a number larger than $k$ (length of sample).

$$
\begin{align}
\mathcal{L}(\bs{\Sigma}\mid \b{x}_1,\dots,\b{x}_n) &=p(\b{x}_1,\dots,\b{x}_n \mid \bs{\mu}, \bs{\Sigma}) \\
&= \prodd (2\pi)^{-k/2}\ \mathrm{det}(\bs{\Sigma})^{-1/2} \, \expb{-\frac{1}{2}(\b{x}_i-\bs{\mu})^\top\bs{\Sigma}^{-1}(\b{x}_i-\bs{\mu})} \\
&\propto \mathrm{det}(\bs{\Sigma})^{-n/2}\, \expb{-\frac{1}{2}\summ (\b{x}_i-\bs{\mu})^\top\bs{\Sigma}^{-1}(\b{x}_i-\bs{\mu})} \\
&\propto \mathrm{det}(\bs{\Sigma})^{-n/2}\, \expb{-\frac{1}{2} \operatorname{tr}\pp{\summ \left[(\b{x}_i-\bs{\mu})^\top(\b{x}_i-\bs{\mu})\right]\bs{\Sigma}^{-1}}} \\
&\propto \mathrm{InverseWishart}\pp{n+p+1, \summ (\b{x}_i-\bs{\mu})^\top(\b{x}_i-\bs{\mu})}
\end{align}
$$
This is the Wishart distribution.
$$
p (\bs{\Lambda}\mid \b{V}) = \frac{1}{2^{np/2} \left|{\mathbf V}\right|^{n/2} \Gamma_p\left(\frac {n}{2}\right ) }{\left|\bs{\Lambda}\right|}^{(n-p-1)/2} \expb{-\frac{1}{2}\operatorname{tr}({\mathbf V}^{-1}\bs{\Lambda})}
$$
However, looking at the likelihood form, every parameter of interest is inverted. Therefore, it is conjugated to the inverse Wishart distribution.

$$
p (\bs{\Sigma}\mid \b{V}) = \frac{1}{2^{np/2} \left|{\mathbf V}\right|^{-n/2} \Gamma_p\left(\frac {n}{2}\right ) }{\left|\bs{\Sigma}\right|}^{-(n-p-1)/2} \expb{-\frac{1}{2}\operatorname{tr}({\mathbf V}\bs{\Sigma}^{-1})}
$$

In the univariate case this would be
??????
$$
\begin{align}
  X &\sim \mathcal{N}(0, \tau_0^2) \\
  \sigma^2 \mid \tau^2_0 &\sim X^2 \\
  &\sim \tau_0^2\, \mathcal{N}(0,1)^2 \\
  &\sim \tau_0^2\, \chi^2_1 \\
  &\sim \tau_0^2\, \mathrm{Gamma}\pp{\frac{1}{2}, \frac{1}{2} } \\
  &\sim \mathrm{Gamma}\pp{\frac{1}{2}, \frac{1}{2\tau_0^2} }
\end{align}
$$
$$
\summ \mathbf{b}_i^\top \mathbf{Ab}_i = \mathrm{tr}( \mathbf{B^\top B A} )  
$$

# Infer Both

Here, we are seeking to infer both the mean and variance of data distributed with a multivariate normal sampling distribution.

## Full conditional of $\bs{\Sigma}$

We know that the sampling distribution of the covariance matrix is conjugated to the inverse Wishart distribution.

$$
\begin{align}
  p(\b{X}_1,\dots,\b{X}_n) &\propto 
\end{align}
$$


## Sampling scheme

$$
\begin{align}
  \b{μ}\mid\b{X}_1,\dots,\b{X}_n, \b{Σ} &\sim \mathrm{MultivariateNormal}(\b{μ}_n, \b{Λ}_n) \\
  \b{Σ} \mid \b{X}_1,\dots,\b{X}_n, \b{μ} &\sim \mathrm{InverseWishart}(n_0+n, \b{Σ}_n^{-1})\\
\end{align}
$$