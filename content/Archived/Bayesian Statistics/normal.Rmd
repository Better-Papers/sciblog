---
title: "Normal Conjugacy"
section: "Bayesian Statistics"
author: "Chaichontat Sriworarat"
date: 2019-10-13
tags: ["bayesian statistics"]
bibliography: [normal.bib]
output:
  blogdown::html_page:
    toc: true
include-before:
  - '$\newcommand{\tst}{\tilde{\sigma}^2}$'
  - '$\newcommand{\summ}{\sum_{i=1}^n}$'
  - '$\newcommand{\prodd}{\prod_{i=1}^n}$'
  - '$\newcommand{\xdots}{x_1, \dots, x_n}$'
  - '$\newcommand{\expb}[1]{\exp\left\{#1\right\}}$'
  - '$\newcommand{\pp}[1]{\left(#1\right)}$'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE)
library(blogdown)
```

# Problem Statement

Let the random variables $X_1, \dots X_n$ be normally distributed.
$$
X_1,\dots, X_n \sim \mathcal{N}(\mu, \sigma^2)\\
$$
With the sampling density function
$$
p(X_i=x_i\mid \mu, \sigma^2) = (2\pi \sigma^2)^{-1/2} \, \exp \left\{ -\frac{(x_i-\mu)^2}{2\sigma^2}\right\}
$$

After the experiment, each realizations of the random variables has been found. The likelihood function is

$$
\begin{align}
p(X_1=x_1,\dots,X_n=x_n\mid\mu,\sigma^2) &= \prod_{i=1}^n (2\pi \sigma^2)^{-1/2} \, \exp \left\{ -\frac{(x_i-\mu)^2}{2\sigma^2}\right\} \\
&= (2\pi \sigma^2)^{-n/2} \, \exp \left\{ -\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\right\}
\end{align}
$$

Since this is a multi-parameter model, lets start with the case in which $\sigma^2$ is known.

# Infer Only $\mu$
Lets find the posterior distribution of $\theta$ given the normally distributed data $X_1=x_1,\dots,X_n=x_n$ with variance $\sigma^2$ and prior distribution $\mu_0$.

Given this likelihood, we expect that the posterior will have the form
$$
\begin{align}
p(\theta \mid x_1,\dots,x_n, \sigma^2) &\propto p(x_1,\dots,x_n\mid \theta, \sigma^2)\ p(\theta) \\
&= (2\pi \sigma^2)^{-n/2} \, \exp \left\{ -\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\theta)^2\right\} \ p(\theta) \\
&\propto \exp \left\{ a\theta^2+b\theta\right\} \ p(\theta)
\end{align}
$$

This suggests that the conjugate prior should have a quadratic exponential term as well and the distribution that has this term is the normal distribution. Let the prior distribution has be $\mathcal{N}(\mu_0, \tau_0^2)$.

$$
\begin{align}
p(\theta \mid x_1,\dots,x_n, \sigma^2) &\propto p(x_1,\dots,x_n\mid \theta, \sigma^2)\ p(\theta \mid \mu_0, \tau^2_0) \\
&= \left[ (2\pi \sigma^2)^{-n/2} \, \exp \left\{ -\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\theta)^2\right\}\right] \left[ (2\pi \tau_0^2)^{-1/2} \, \exp \left\{ -\frac{(\theta-\mu_0)^2}{2\tau_0^2}\right\} \right]\\
&\propto \exp\left\{ -\frac{1}{2}\left[\frac{1}{\sigma^2}\left(\sum_{i=1}^n x_i^2 - 2\theta \sum_{i=1}^n x_i +n\theta^2 \right)+\frac{1}{\tau_0^2}( \theta^2 -2\theta\mu_0 + \mu_0^2)\right]\right\} \\
&\propto \exp\left\{ -\frac{1}{2}\left[\left(\frac{n}{\sigma^2}+\frac{1}{\tau_0^2}\right)\theta^2- 2\left( \frac{n \bar{x}}{\sigma^2} +\frac{\mu_0}{\tau_0^2} \right)\theta\right]\right\} \\
\end{align}
$$

Let $a=\frac{n}{\sigma^2} +\frac{1}{\tau_0^2}$ and $b=\frac{n \bar{x}}{\sigma^2} +\frac{\mu_0}{\tau_0^2}$.

$$
\begin{align}
&\propto \exp\left\{ -\frac{1}{2}(a\theta^2- 2b\theta)\right\} \\
&\propto \exp\left\{ -\frac{a}{2}\left(\theta - \frac{b}{a}\right)^2\right\} \\
&\propto \mathcal{N}\left(\frac{b}{a}, \frac{1}{a}\right)
\end{align}
$$

It turns out that working with the inverse of the variance, or the _precision_ is much more convenient. We will denote $\tilde{\sigma}^2$ and $\tilde{\tau_0}^2$ as the precision,

Then, the posterior mean of the mean $\theta$ is
$$
\begin{align}
\frac{b}{a} &= \frac{\frac{n \bar{x}}{\sigma^2} +\frac{\mu_0}{\tau_0^2}}{\frac{n}{\sigma^2} +\frac{1}{\tau_0^2}} \\
&= \frac{n\bar{x}\tilde{\sigma}^2 + \mu_0\tilde{\tau}_0^2}{n\tilde{\sigma}^2+\tilde{\tau}_0^2} \\
&= \frac{\tilde{\sigma}^2}{n\tilde{\sigma}^2+\tilde{\tau}_0^2}n\bar{x} + \frac{\tilde{\tau}_0^2}{n\tilde{\sigma}^2+\tilde{\tau}_0^2}\mu_0
\end{align}
$$

which means that

The posterior precision is
$$
\begin{align}
\tilde{\tau}_0^2 = \frac{1}{\tau_0^2} &= \frac{n}{\sigma^2} +\frac{1}{\tau_0^2}
\end{align}
$$

In summary, the normal sampling distribution has a mean distribution of $\theta$ and variance $\sigma^2$. $\theta$ is distributed as followed.
$$
\theta \sim \mathcal{N}\left( \frac{\tilde{\sigma}^2}{n\tilde{\sigma}^2+\tilde{\tau}_0^2}n\bar{x} + \frac{\tilde{\tau}_0^2}{n\tilde{\sigma}^2+\tilde{\tau}_0^2}\mu_0, \left(\frac{n}{\sigma^2} +\frac{1}{\tau_0^2}\right)^{-1} \right)
$$

# Infer Only $\sigma^2$
Lets find the posterior distribution of $\sigma^2$ given the normally distributed data $X_1=x_1,\dots,X_n=x_n$ with mean $\mu$ with $\sigma^2$ having a prior distribution $p(\sigma_0^2)$.

Given this likelihood, we expect that the posterior will have the form
$$
\begin{align}
p(\sigma^2 \mid x_1,\dots,x_n, \mu) &\propto p(x_1,\dots,x_n\mid \mu, \sigma^2)\ p(\sigma^2) \\
&= (2\pi \sigma^2)^{-n/2} \, \exp \left\{ -\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\right\} \ p(\sigma^2) \\
&\propto (\sigma^2)^{-n/2} \exp \left\{-\frac{a}{\sigma^2} \right\} \ p(\sigma^2) \\
\end{align}
$$

This matches the form of an inverse gamma distribution. If we reparameterize in terms of precision, we get,
$$
\propto (\tilde{\sigma}^2)^{n/2} \exp \left\{-a\tilde{\sigma}^2 \right\} \ p(\theta)
$$

which fits the gamma distribution parameterized with shape and **rate**. Then

$$
\begin{align}
p(\tst \mid \xdots, \mu) &\propto p(\xdots \mid \mu)\ p(\tst\mid \alpha, \beta) \\
&\propto \left[ (\tst)^{n/2} \, \expb{-\frac{\tst}{2} \summ(x_i-\mu)^2} \right]\, \left[(\tst)^{\alpha-1}\, \expb{-\beta\tst}\right] \\
&\propto (\tst)^{n/2+\alpha-1} \, \expb{-\tst\left[\frac{1}{2} \summ(x_i-\mu)^2 + \beta\right]} \\
&\propto (\tst)^{n/2+\alpha-1} \, \expb{-\tst\left[\frac{1}{2} \summ((x_i-\bar{x})+(\bar{x}-\mu))^2 + \beta\right]} \\
&\propto (\tst)^{n/2+\alpha-1} \, \expb{-\tst\left[\frac{1}{2} \left[\summ(x_i-\bar{x})^2 - 2\summ(x_i-\bar{x})(\bar{x}-\mu)+\summ(\bar{x}-\mu)^2\right] + \beta\right]} \\
&\propto (\tst)^{n/2+\alpha-1} \, \expb{-\tst\left[\frac{1}{2} \left[\summ(x_i-\bar{x})^2 +\summ(\bar{x}-\mu)^2\right] + \beta\right]} \\
&\propto (\tst)^{n/2+\alpha-1} \, \expb{-\tst\left[\frac{1}{2} \left[(n-1)s^2_x +n(\bar{x}-\mu)^2\right] + \beta\right]} \\
&\propto \mathrm{Gamma}\left(\frac{n}{2}+\alpha, \frac{(n-1)s^2_x +n(\bar{x}-\mu)^2}{2} + \beta\right)
\end{align}
$$
If we define $\alpha = \frac{n_0}{2}$ and $\beta=\frac{n_0\sigma_0^2}{2}$, then
$$
p(\tst \mid \xdots, \mu) \sim \mathrm{Gamma}\pp{\frac{n+n_0}{2}, \frac{(n-1)s^2_x +n(\bar{x}-\mu)^2+n_0\sigma_0^2}{2}}  \> \text{(rate)}
$$

### Interpretations
* $n_0$: sample size where $\sigma_0^2$ is obtained.
* $(n-1)s_x^2+n_0\sigma_0^2$: sum of sum of sqaures of the prior sample and current sample.

# Infer Both $\mu$ and $\sigma^2$

Now, let us infer both parameters together. The posterior distribution would be a joint distribution.

$$
\begin{align}
p(\mu,\sigma^2\mid \xdots) &= p(\mu\mid\sigma^2,\xdots)\ p(\sigma^2,\xdots) \\
\end{align}
$$
Notice that we can factor out the joint distribution into two posteriors. Let us examine each posterior.

$$
\begin{align}
p(\mu\mid\sigma^2,\xdots) &\propto p(\xdots \mid \mu, \sigma^2) \ p(\mu\mid\sigma^2) \\
&\propto \mathcal{N}\pp{\mu\mid\xdots, \sigma^2}\ \mathcal{N}\pp{\mu\mid\mu_0,\frac{\sigma^2}{n_{0,\mu}}}\\
&= \mathcal{N}\left( \frac{n\tst}{n\tst + n_{0,\mu}\tst} \bar{x} + \frac{n_{0,\mu}\tst}{n\tst + n_{0,\mu}\tst} \mu_0, \left(\frac{n}{\sigma^2} +\frac{n_{0,\mu}}{\sigma^2}\right)^{-1} \right) \\
&= \mathcal{N}\pp{\frac{n\bar{x}+\mu_0 n_{0,\mu}}{n+n_{0,\mu}}, (n+n_{0,\mu})\tst}
\end{align}
$$

Let us examine the second posterior.
$$
\begin{align}
p(\sigma^2,\xdots) &\propto p(\xdots\mid\sigma^2) \ p(\sigma^2) \\
&= \left[\int p(\xdots \mid \theta,\sigma^2)\ p(\theta\mid \sigma^2) \, d\theta\right]\ p(\sigma^2) \\
\frac{1}{\sigma^2} &\sim \mathrm{Gamma}\pp{\frac{n_{0,\sigma^2}+n}{2}, \frac{1}{n_{0,\sigma^2}+n}\left[n_{0,\sigma^2}\sigma_0^2+(n-1)s_x^2+\frac{n_{0,\mu}\, n}{n_{0,\mu}}(\bar{x}-\mu_0)^2\right]}   \> \text{(rate)} \\
\end{align}
$$
If we assume that $n_{0,\sigma^2} = n_{0,\mu}$.
$$
\frac{1}{\sigma^2} \sim \mathrm{Gamma}\pp{\frac{n_{0}+n}{2}, \frac{1}{n_{0}+n}\left[n_{0}\sigma_0^2+(n-1)s_x^2+ n(\bar{x}-\mu_0)^2\right]}\> \text{(rate)}
$$

### Interpretations
* $n_{0,\sigma^2}$: sample size in which the variance $\sigma_0^2$ is obtained.
* $n_{0,\mu}$: sample size in which the mean $\mu_0$ is obtained.
* $n_{0,\sigma^2}\sigma_0^2+(n-1)s^2$: sum of sums of squares for prior and current sample.
* $n(\bar{x}-\mu_0)^2$: "interaction term". If our current mean is different from the prior mean, then our uncertainty should be higher.


To recap, the priors are as followed.
$$
\begin{align}
\frac{1}{\sigma^2} &\sim \mathrm{Gamma}\pp{\frac{n_{0,\sigma^2}}{2}, \frac{n_{0,\sigma^2}\sigma_0^2}{2}}   \> \text{(rate)} \\
\mu\mid\sigma^2 &\sim \mathcal{N}\pp{\mu_0, \frac{\sigma^2}{n_{0,\mu^2}}}
\end{align}
$$
The posteriors are
$$
\begin{align}
\frac{1}{\sigma^2} &\sim \mathrm{Gamma}\pp{\frac{n_{0,\sigma^2}+n}{2}, \frac{1}{n_{0,\sigma^2}+n}\left[n_{0,\sigma^2}\sigma_0^2+(n-1)s^2+\frac{n_{0,\mu}\, n}{n_{0,\mu}}(\bar{x}-\mu_0)^2\right]}   \> \text{(rate)} \\
\mu\mid\sigma^2 &\sim \mathcal{N}\pp{\frac{n\bar{x}+\mu_0 n_{0,\mu}}{n+n_{0,\mu}}, (n+n_{0,\mu})\tst}
\end{align}
$$

Now we can sample this sequentially to get a joint distribution.

# Independent Priors
In the previous section, the prior is modeled as another sample set, which necessitates the mean and variance to be dependent on one another. However, if we want to model each parameter $\mu$ and $\sigma^2$ separately, then conjugacy breaks down. The priors are

$$
\begin{align}
\frac{1}{\sigma^2} &\sim \mathrm{Gamma}\pp{\frac{n_{0,\sigma^2}}{2}, \frac{n_{0,\sigma^2}\sigma_0^2}{2}}   \> \text{(rate)}\\
\mu &\sim \mathcal{N}\pp{\mu_0, \tau_0^2}
\end{align}
$$

Still, it is possible to derive the joint distribution of the posterior by dependent sampling. Note that the each conjugate prior is conjugated to the full conditional of each parameter.

$$
\begin{align}
p(\sigma^2\mid \theta, \xdots) &\propto p(\theta,\sigma^2,\xdots) \\
&\propto p(\xdots \mid \theta,\sigma^2)\ p(\theta,\sigma^2) \\
&= p(\xdots \mid \theta,\sigma^2)\ p(\theta\mid\sigma^2) \ p(\sigma^2)\\
&= p(\xdots \mid \theta,\sigma^2)\ p(\theta) \ p(\sigma^2)\\
\end{align}
$$
Then, note that for all combinations of $\theta$ and $\xdots$, $p(\sigma^2\mid \theta, \xdots)$ has to be a probability distribution function. In each case, however, since $\theta$ and $\xdots$ are fixed, we can push them into our 'proportional to' trash can! This is exactly the same case when we're deriving one parameter at a time.

$$
\begin{align}
p(\sigma^2\mid \theta, \xdots) &\propto p(\xdots \mid \theta,\sigma^2)\ p(\sigma^2)\\
&\propto \mathrm{Gamma}\pp{\frac{n+n_0}{2}, \frac{(n-1)s^2_x +n(\bar{x}-\mu)^2+n_0\sigma_0^2}{2}} \> \text{(rate)} \\~\\
p(\theta\mid \sigma^2, \xdots) &\propto p(\xdots \mid \theta,\sigma^2)\ p(\theta)\\
&\propto \mathcal{N}\left( \frac{\tilde{\sigma}^2}{n\tilde{\sigma}^2+\tilde{\tau}_0^2}n\bar{x} + \frac{\tilde{\tau}_0^2}{n\tilde{\sigma}^2+\tilde{\tau}_0^2}\mu_0, \left(\frac{n}{\sigma^2} +\frac{1}{\tau_0^2}\right)^{-1} \right)
\end{align}
$$

[@berkeley]
