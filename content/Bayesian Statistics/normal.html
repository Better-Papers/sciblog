---
title: "Normal Conjugacy"
section: "Bayesian Statistics"
author: "Chaichontat Sriworarat"
date: 2019-10-13
tags: ["bayesian statistics"]
bibliography: [normal.bib]
output:
  blogdown::html_page:
    toc: true
include-before:
  - '$\newcommand{\tst}{\tilde{\sigma}^2}$'
  - '$\newcommand{\summ}{\sum_{i=1}^n}$'
  - '$\newcommand{\prodd}{\prod_{i=1}^n}$'
  - '$\newcommand{\xdots}{x_1, \dots, x_n}$'
  - '$\newcommand{\expb}[1]{\exp\left\{#1\right\}}$'
  - '$\newcommand{\pp}[1]{\left(#1\right)}$'
---


<span class="math inline">\(\newcommand{\tst}{\tilde{\sigma}^2}\)</span>
<span class="math inline">\(\newcommand{\summ}{\sum_{i=1}^n}\)</span>
<span class="math inline">\(\newcommand{\prodd}{\prod_{i=1}^n}\)</span>
<span class="math inline">\(\newcommand{\xdots}{x_1, \dots, x_n}\)</span>
<span class="math inline">\(\newcommand{\expb}[1]{\exp\left\{#1\right\}}\)</span>
<span class="math inline">\(\newcommand{\pp}[1]{\left(#1\right)}\)</span>
<div id="TOC">
<ul>
<li><a href="#problem-statement">Problem Statement</a></li>
<li><a href="#infer-only-mu">Infer Only <span class="math inline">\(\mu\)</span></a></li>
<li><a href="#infer-only-sigma2">Infer Only <span class="math inline">\(\sigma^2\)</span></a><ul>
<li><a href="#interpretations">Interpretations</a></li>
</ul></li>
<li><a href="#infer-both-mu-and-sigma2">Infer Both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span></a><ul>
<li><a href="#interpretations-1">Interpretations</a></li>
</ul></li>
<li><a href="#independent-priors">Independent Priors</a></li>
</ul>
</div>

<div id="problem-statement" class="section level1">
<h1>Problem Statement</h1>
<p>Let the random variables <span class="math inline">\(X_1, \dots X_n\)</span> be normally distributed.
<span class="math display">\[
X_1,\dots, X_n \sim \mathcal{N}(\mu, \sigma^2)\\
\]</span>
With the sampling density function
<span class="math display">\[
p(X_i=x_i\mid \mu, \sigma^2) = (2\pi \sigma^2)^{-1/2} \, \exp \left\{ -\frac{(x_i-\mu)^2}{2\sigma^2}\right\}
\]</span></p>
<p>After the experiment, each realizations of the random variables has been found. The likelihood function is</p>
<p><span class="math display">\[
\begin{align}
p(X_1=x_1,\dots,X_n=x_n\mid\mu,\sigma^2) &amp;= \prod_{i=1}^n (2\pi \sigma^2)^{-1/2} \, \exp \left\{ -\frac{(x_i-\mu)^2}{2\sigma^2}\right\} \\
&amp;= (2\pi \sigma^2)^{-n/2} \, \exp \left\{ -\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\right\}
\end{align}
\]</span></p>
<p>Since this is a multi-parameter model, lets start with the case in which <span class="math inline">\(\sigma^2\)</span> is known.</p>
</div>
<div id="infer-only-mu" class="section level1">
<h1>Infer Only <span class="math inline">\(\mu\)</span></h1>
<p>Lets find the posterior distribution of <span class="math inline">\(\theta\)</span> given the normally distributed data <span class="math inline">\(X_1=x_1,\dots,X_n=x_n\)</span> with variance <span class="math inline">\(\sigma^2\)</span> and prior distribution <span class="math inline">\(\mu_0\)</span>.</p>
<p>Given this likelihood, we expect that the posterior will have the form
<span class="math display">\[
\begin{align}
p(\theta \mid x_1,\dots,x_n, \sigma^2) &amp;\propto p(x_1,\dots,x_n\mid \theta, \sigma^2)\ p(\theta) \\
&amp;= (2\pi \sigma^2)^{-n/2} \, \exp \left\{ -\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\theta)^2\right\} \ p(\theta) \\
&amp;\propto \exp \left\{ a\theta^2+b\theta\right\} \ p(\theta)
\end{align}
\]</span></p>
<p>This suggests that the conjugate prior should have a quadratic exponential term as well and the distribution that has this term is the normal distribution. Let the prior distribution has be <span class="math inline">\(\mathcal{N}(\mu_0, \tau_0^2)\)</span>.</p>
<p><span class="math display">\[
\begin{align}
p(\theta \mid x_1,\dots,x_n, \sigma^2) &amp;\propto p(x_1,\dots,x_n\mid \theta, \sigma^2)\ p(\theta \mid \mu_0, \tau^2_0) \\
&amp;= \left[ (2\pi \sigma^2)^{-n/2} \, \exp \left\{ -\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\theta)^2\right\}\right] \left[ (2\pi \tau_0^2)^{-1/2} \, \exp \left\{ -\frac{(\theta-\mu_0)^2}{2\tau_0^2}\right\} \right]\\
&amp;\propto \exp\left\{ -\frac{1}{2}\left[\frac{1}{\sigma^2}\left(\sum_{i=1}^n x_i^2 - 2\theta \sum_{i=1}^n x_i +n\theta^2 \right)+\frac{1}{\tau_0^2}( \theta^2 -2\theta\mu_0 + \mu_0^2)\right]\right\} \\
&amp;\propto \exp\left\{ -\frac{1}{2}\left[\left(\frac{n}{\sigma^2}+\frac{1}{\tau_0^2}\right)\theta^2- 2\left( \frac{n \bar{x}}{\sigma^2} +\frac{\mu_0}{\tau_0^2} \right)\theta\right]\right\} \\
\end{align}
\]</span></p>
<p>Let <span class="math inline">\(a=\frac{n}{\sigma^2} +\frac{1}{\tau_0^2}\)</span> and <span class="math inline">\(b=\frac{n \bar{x}}{\sigma^2} +\frac{\mu_0}{\tau_0^2}\)</span>.</p>
<p><span class="math display">\[
\begin{align}
&amp;\propto \exp\left\{ -\frac{1}{2}(a\theta^2- 2b\theta)\right\} \\
&amp;\propto \exp\left\{ -\frac{a}{2}\left(\theta - \frac{b}{a}\right)^2\right\} \\
&amp;\propto \mathcal{N}\left(\frac{b}{a}, \frac{1}{a}\right)
\end{align}
\]</span></p>
<p>It turns out that working with the inverse of the variance, or the <em>precision</em> is much more convenient. We will denote <span class="math inline">\(\tilde{\sigma}^2\)</span> and <span class="math inline">\(\tilde{\tau_0}^2\)</span> as the precision,</p>
<p>Then, the posterior mean of the mean <span class="math inline">\(\theta\)</span> is
<span class="math display">\[
\begin{align}
\frac{b}{a} &amp;= \frac{\frac{n \bar{x}}{\sigma^2} +\frac{\mu_0}{\tau_0^2}}{\frac{n}{\sigma^2} +\frac{1}{\tau_0^2}} \\
&amp;= \frac{n\bar{x}\tilde{\sigma}^2 + \mu_0\tilde{\tau}_0^2}{n\tilde{\sigma}^2+\tilde{\tau}_0^2} \\
&amp;= \frac{\tilde{\sigma}^2}{n\tilde{\sigma}^2+\tilde{\tau}_0^2}n\bar{x} + \frac{\tilde{\tau}_0^2}{n\tilde{\sigma}^2+\tilde{\tau}_0^2}\mu_0
\end{align}
\]</span></p>
<p>which means that</p>
<p>The posterior precision is
<span class="math display">\[
\begin{align}
\tilde{\tau}_0^2 = \frac{1}{\tau_0^2} &amp;= \frac{n}{\sigma^2} +\frac{1}{\tau_0^2}
\end{align}
\]</span></p>
<p>In summary, the normal sampling distribution has a mean distribution of <span class="math inline">\(\theta\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. <span class="math inline">\(\theta\)</span> is distributed as followed.
<span class="math display">\[
\theta \sim \mathcal{N}\left( \frac{\tilde{\sigma}^2}{n\tilde{\sigma}^2+\tilde{\tau}_0^2}n\bar{x} + \frac{\tilde{\tau}_0^2}{n\tilde{\sigma}^2+\tilde{\tau}_0^2}\mu_0, \left(\frac{n}{\sigma^2} +\frac{1}{\tau_0^2}\right)^{-1} \right)
\]</span></p>
</div>
<div id="infer-only-sigma2" class="section level1">
<h1>Infer Only <span class="math inline">\(\sigma^2\)</span></h1>
<p>Lets find the posterior distribution of <span class="math inline">\(\sigma^2\)</span> given the normally distributed data <span class="math inline">\(X_1=x_1,\dots,X_n=x_n\)</span> with mean <span class="math inline">\(\mu\)</span> with <span class="math inline">\(\sigma^2\)</span> having a prior distribution <span class="math inline">\(p(\sigma_0^2)\)</span>.</p>
<p>Given this likelihood, we expect that the posterior will have the form
<span class="math display">\[
\begin{align}
p(\sigma^2 \mid x_1,\dots,x_n, \mu) &amp;\propto p(x_1,\dots,x_n\mid \mu, \sigma^2)\ p(\sigma^2) \\
&amp;= (2\pi \sigma^2)^{-n/2} \, \exp \left\{ -\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\right\} \ p(\sigma^2) \\
&amp;\propto (\sigma^2)^{-n/2} \exp \left\{-\frac{a}{\sigma^2} \right\} \ p(\sigma^2) \\
\end{align}
\]</span></p>
<p>This matches the form of an inverse gamma distribution. If we reparameterize in terms of precision, we get,
<span class="math display">\[
\propto (\tilde{\sigma}^2)^{n/2} \exp \left\{-a\tilde{\sigma}^2 \right\} \ p(\theta)
\]</span></p>
<p>which fits the gamma distribution parameterized with shape and <strong>rate</strong>. Then</p>
<p><span class="math display">\[
\begin{align}
p(\tst \mid \xdots, \mu) &amp;\propto p(\xdots \mid \mu)\ p(\tst\mid \alpha, \beta) \\
&amp;\propto \left[ (\tst)^{n/2} \, \expb{-\frac{\tst}{2} \summ(x_i-\mu)^2} \right]\, \left[(\tst)^{\alpha-1}\, \expb{-\beta\tst}\right] \\
&amp;\propto (\tst)^{n/2+\alpha-1} \, \expb{-\tst\left[\frac{1}{2} \summ(x_i-\mu)^2 + \beta\right]} \\
&amp;\propto (\tst)^{n/2+\alpha-1} \, \expb{-\tst\left[\frac{1}{2} \summ((x_i-\bar{x})+(\bar{x}-\mu))^2 + \beta\right]} \\
&amp;\propto (\tst)^{n/2+\alpha-1} \, \expb{-\tst\left[\frac{1}{2} \left[\summ(x_i-\bar{x})^2 - 2\summ(x_i-\bar{x})(\bar{x}-\mu)+\summ(\bar{x}-\mu)^2\right] + \beta\right]} \\
&amp;\propto (\tst)^{n/2+\alpha-1} \, \expb{-\tst\left[\frac{1}{2} \left[\summ(x_i-\bar{x})^2 +\summ(\bar{x}-\mu)^2\right] + \beta\right]} \\
&amp;\propto (\tst)^{n/2+\alpha-1} \, \expb{-\tst\left[\frac{1}{2} \left[(n-1)s^2_x +n(\bar{x}-\mu)^2\right] + \beta\right]} \\
&amp;\propto \mathrm{Gamma}\left(\frac{n}{2}+\alpha, \frac{(n-1)s^2_x +n(\bar{x}-\mu)^2}{2} + \beta\right)
\end{align}
\]</span>
If we define <span class="math inline">\(\alpha = \frac{n_0}{2}\)</span> and <span class="math inline">\(\beta=\frac{n_0\sigma_0^2}{2}\)</span>, then
<span class="math display">\[
p(\tst \mid \xdots, \mu) \sim \mathrm{Gamma}\pp{\frac{n+n_0}{2}, \frac{(n-1)s^2_x +n(\bar{x}-\mu)^2+n_0\sigma_0^2}{2}}  \&gt; \text{(rate)}
\]</span></p>
<div id="interpretations" class="section level3">
<h3>Interpretations</h3>
<ul>
<li><span class="math inline">\(n_0\)</span>: sample size where <span class="math inline">\(\sigma_0^2\)</span> is obtained.</li>
<li><span class="math inline">\((n-1)s_x^2+n_0\sigma_0^2\)</span>: sum of sum of sqaures of the prior sample and current sample.</li>
</ul>
</div>
</div>
<div id="infer-both-mu-and-sigma2" class="section level1">
<h1>Infer Both <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span></h1>
<p>Now, let us infer both parameters together. The posterior distribution would be a joint distribution.</p>
<p><span class="math display">\[
\begin{align}
p(\mu,\sigma^2\mid \xdots) &amp;= p(\mu\mid\sigma^2,\xdots)\ p(\sigma^2,\xdots) \\
\end{align}
\]</span>
Notice that we can factor out the joint distribution into two posteriors. Let us examine each posterior.</p>
<p><span class="math display">\[
\begin{align}
p(\mu\mid\sigma^2,\xdots) &amp;\propto p(\xdots \mid \mu, \sigma^2) \ p(\mu\mid\sigma^2) \\
&amp;\propto \mathcal{N}\pp{\mu\mid\xdots, \sigma^2}\ \mathcal{N}\pp{\mu\mid\mu_0,\frac{\sigma^2}{n_{0,\mu}}}\\
&amp;= \mathcal{N}\left( \frac{n\tst}{n\tst + n_{0,\mu}\tst} \bar{x} + \frac{n_{0,\mu}\tst}{n\tst + n_{0,\mu}\tst} \mu_0, \left(\frac{n}{\sigma^2} +\frac{n_{0,\mu}}{\sigma^2}\right)^{-1} \right) \\
&amp;= \mathcal{N}\pp{\frac{n\bar{x}+\mu_0 n_{0,\mu}}{n+n_{0,\mu}}, (n+n_{0,\mu})\tst}
\end{align}
\]</span></p>
<p>Let us examine the second posterior.
<span class="math display">\[
\begin{align}
p(\sigma^2,\xdots) &amp;\propto p(\xdots\mid\sigma^2) \ p(\sigma^2) \\
&amp;= \left[\int p(\xdots \mid \theta,\sigma^2)\ p(\theta\mid \sigma^2) \, d\theta\right]\ p(\sigma^2) \\
\frac{1}{\sigma^2} &amp;\sim \mathrm{Gamma}\pp{\frac{n_{0,\sigma^2}+n}{2}, \frac{1}{n_{0,\sigma^2}+n}\left[n_{0,\sigma^2}\sigma_0^2+(n-1)s_x^2+\frac{n_{0,\mu}\, n}{n_{0,\mu}}(\bar{x}-\mu_0)^2\right]}   \&gt; \text{(rate)} \\
\end{align}
\]</span>
If we assume that <span class="math inline">\(n_{0,\sigma^2} = n_{0,\mu}\)</span>.
<span class="math display">\[
\frac{1}{\sigma^2} \sim \mathrm{Gamma}\pp{\frac{n_{0}+n}{2}, \frac{1}{n_{0}+n}\left[n_{0}\sigma_0^2+(n-1)s_x^2+ n(\bar{x}-\mu_0)^2\right]}\&gt; \text{(rate)}
\]</span></p>
<div id="interpretations-1" class="section level3">
<h3>Interpretations</h3>
<ul>
<li><span class="math inline">\(n_{0,\sigma^2}\)</span>: sample size in which the variance <span class="math inline">\(\sigma_0^2\)</span> is obtained.</li>
<li><span class="math inline">\(n_{0,\mu}\)</span>: sample size in which the mean <span class="math inline">\(\mu_0\)</span> is obtained.</li>
<li><span class="math inline">\(n_{0,\sigma^2}\sigma_0^2+(n-1)s^2\)</span>: sum of sums of squares for prior and current sample.</li>
<li><span class="math inline">\(n(\bar{x}-\mu_0)^2\)</span>: “interaction term”. If our current mean is different from the prior mean, then our uncertainty should be higher.</li>
</ul>
<p>To recap, the priors are as followed.
<span class="math display">\[
\begin{align}
\frac{1}{\sigma^2} &amp;\sim \mathrm{Gamma}\pp{\frac{n_{0,\sigma^2}}{2}, \frac{n_{0,\sigma^2}\sigma_0^2}{2}}   \&gt; \text{(rate)} \\
\mu\mid\sigma^2 &amp;\sim \mathcal{N}\pp{\mu_0, \frac{\sigma^2}{n_{0,\mu^2}}}
\end{align}
\]</span>
The posteriors are
<span class="math display">\[
\begin{align}
\frac{1}{\sigma^2} &amp;\sim \mathrm{Gamma}\pp{\frac{n_{0,\sigma^2}+n}{2}, \frac{1}{n_{0,\sigma^2}+n}\left[n_{0,\sigma^2}\sigma_0^2+(n-1)s^2+\frac{n_{0,\mu}\, n}{n_{0,\mu}}(\bar{x}-\mu_0)^2\right]}   \&gt; \text{(rate)} \\
\mu\mid\sigma^2 &amp;\sim \mathcal{N}\pp{\frac{n\bar{x}+\mu_0 n_{0,\mu}}{n+n_{0,\mu}}, (n+n_{0,\mu})\tst}
\end{align}
\]</span></p>
<p>Now we can sample this sequentially to get a joint distribution.</p>
</div>
</div>
<div id="independent-priors" class="section level1">
<h1>Independent Priors</h1>
<p>In the previous section, the prior is modeled as another sample set, which necessitates the mean and variance to be dependent on one another. However, if we want to model each parameter <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> separately, then conjugacy breaks down. The priors are</p>
<p><span class="math display">\[
\begin{align}
\frac{1}{\sigma^2} &amp;\sim \mathrm{Gamma}\pp{\frac{n_{0,\sigma^2}}{2}, \frac{n_{0,\sigma^2}\sigma_0^2}{2}}   \&gt; \text{(rate)}\\
\mu &amp;\sim \mathcal{N}\pp{\mu_0, \tau_0^2}
\end{align}
\]</span></p>
<p>Still, it is possible to derive the joint distribution of the posterior by dependent sampling. Note that the each conjugate prior is conjugated to the full conditional of each parameter.</p>
<p><span class="math display">\[
\begin{align}
p(\sigma^2\mid \theta, \xdots) &amp;\propto p(\theta,\sigma^2,\xdots) \\
&amp;\propto p(\xdots \mid \theta,\sigma^2)\ p(\theta,\sigma^2) \\
&amp;= p(\xdots \mid \theta,\sigma^2)\ p(\theta\mid\sigma^2) \ p(\sigma^2)\\
&amp;= p(\xdots \mid \theta,\sigma^2)\ p(\theta) \ p(\sigma^2)\\
\end{align}
\]</span>
Then, note that for all combinations of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\xdots\)</span>, <span class="math inline">\(p(\sigma^2\mid \theta, \xdots)\)</span> has to be a probability distribution function. In each case, however, since <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\xdots\)</span> are fixed, we can push them into our ‘proportional to’ trash can! This is exactly the same case when we’re deriving one parameter at a time.</p>
<p><span class="math display">\[
\begin{align}
p(\sigma^2\mid \theta, \xdots) &amp;\propto p(\xdots \mid \theta,\sigma^2)\ p(\sigma^2)\\
&amp;\propto \mathrm{Gamma}\pp{\frac{n+n_0}{2}, \frac{(n-1)s^2_x +n(\bar{x}-\mu)^2+n_0\sigma_0^2}{2}} \&gt; \text{(rate)} \\~\\
p(\theta\mid \sigma^2, \xdots) &amp;\propto p(\xdots \mid \theta,\sigma^2)\ p(\theta)\\
&amp;\propto \mathcal{N}\left( \frac{\tilde{\sigma}^2}{n\tilde{\sigma}^2+\tilde{\tau}_0^2}n\bar{x} + \frac{\tilde{\tau}_0^2}{n\tilde{\sigma}^2+\tilde{\tau}_0^2}\mu_0, \left(\frac{n}{\sigma^2} +\frac{1}{\tau_0^2}\right)^{-1} \right)
\end{align}
\]</span></p>
<p><span class="citation">(<em>The Conjugate Prior for the Normal Distribution</em>, n.d.)</span></p>
<div id="refs" class="references">
<div id="ref-berkeley">
<p><em>The Conjugate Prior for the Normal Distribution</em>. n.d. <a href="https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/lectures/lecture5.pdf">https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/lectures/lecture5.pdf</a>.</p>
</div>
</div>
</div>
